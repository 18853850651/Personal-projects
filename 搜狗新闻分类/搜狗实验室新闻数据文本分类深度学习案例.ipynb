{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搜狗实验室新闻数据文本分类深度学习案例\n",
    "### ———Tensorflow+CNN深度学习全流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、项目简介\n",
    "tensorflow是谷歌开源的深度学习框架，是进行深度学习的坚船利炮。此文基于“搜狗实验数据库”的海量新闻数据，全流程展示如何基于tensorflow采用CNN算法实现文章的分类。方便学习者全面地理解深度学习及NLP文本分析的原理和实现步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、数据预处理\n",
    "* 机器学习中，最为基础也是最为耗时的一项工作就是数据预处理。如何将海量数据进行预处理，进而得到数据处理和机器学习阶段所需要的有效素材是一项非常重要的工作。\n",
    "* 此文将以海量新闻xml原始数据处理为例，展示如何有效第进行数据预处理工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、了解原始数据特征\n",
    "* 该数据来源于“搜狗实验室数据库”的“全网新闻数据”http://www.sogou.com/labs/resource/cs.php\n",
    "* 该数据是直接爬取的网页xml格式信息，存在大量的格式标签文件(见下图)\n",
    "* 因此，第一步就是提取xml格式中‘<content>’标签内的有效数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、xml格式有效信息提取\n",
    "* xml格式信息的<docs></docs>标签缺失，首先需要补齐\n",
    "* 利用urllib工具进行xml文件解析，获取url（域名）和content（正文）标签信息\n",
    "* url中的\"&\"要用\"&amp;\"替换才能解析url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,random\n",
    "from xml.dom import minidom\n",
    "from urllib.parse import urlparse\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理xml文件，解决docs,url,content标签问题\n",
    "def file_fill(file_dir,half_dir):\n",
    "    # 查看half_dir文件夹下的文件夹和文件目录\n",
    "    for _,_,files in os.walk(file_dir):\n",
    "        # 判断是否存在half_dir，如果没有则创建\n",
    "        if not os.path.exists(half_dir):\n",
    "            os.makedirs(half_dir)\n",
    "        for f in files:\n",
    "            tmp_dir = half_dir+'/'+f\n",
    "            text_init_dir = file_dir+'/'+f\n",
    "            # 遍历文件夹下的每一篇xml文件\n",
    "            with open(text_init_dir, 'r', encoding='gb18030') as source_file:\n",
    "                start,end = '<docs>\\n','</docs>'\n",
    "                line_content = source_file.readlines()\n",
    "                # 在目标文件夹中创建新文件保存预处理后的文件\n",
    "                with open(tmp_dir, 'w+', encoding='utf-8') as handle_file:\n",
    "                    # 添加'<docs>'头标签\n",
    "                    handle_file.write(start)\n",
    "                    for line in line_content:\n",
    "                        # 处理url中的‘&’符号\n",
    "                        text = line.replace('&', '&amp;')\n",
    "                        # 添加'</docs>'头标签\n",
    "                        handle_file.write(text)\n",
    "                    handle_file.write(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、定义labels和check_class分类判别函数\n",
    "* labels为url与文章类别的映射字典\n",
    "* check_class则直接判断url是否在分类字典中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立url和类别的映射字典\n",
    "# 经过样本分析已获知，数据主要分为以下几类\n",
    "labels = {'auto':'汽车','tech':'科技',\n",
    "          'health':'健康','sports':'体育',\n",
    "          'house':'房产','edu':'教育',\n",
    "          'lady':'女人','eladies':'女人',\n",
    "          'mil':'军事','military':'军事',\n",
    "          'money':'财经','finance':'财经',\n",
    "          'cul':'文化','culture':'文化',\n",
    "          '2008':'奥运'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查url对应的文章是否在分类字典中\n",
    "def check_class(url_lb,labels):\n",
    "    if url_lb in labels:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、运行程序，获取预处理后的数据\n",
    "* 获取预处理后的xml文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = './SogouCA.reduced'\n",
    "half_dir = './sougou_half'\n",
    "file_fill(file_dir,half_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5、获取部分样本文件\n",
    "* 由于数据量太大，进行简化处理，\n",
    "* 从half_dir中随机选取n个文件存入after_dir文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部分样本获取函数\n",
    "def choice_files(half_dir,choice_dir,n):\n",
    "    if not os.path.exists(choice_dir):\n",
    "        os.makedirs(choice_dir)\n",
    "    for _,_,files in os.walk(half_dir):\n",
    "        file_list = random.sample(files,n)\n",
    "        for file in file_list:\n",
    "            with open(half_dir+'./'+file,'r',encoding='utf-8') as f1:\n",
    "                doc = f1.read()\n",
    "                path = './'+choice_dir+'./'+file\n",
    "                with open(path,'a+',encoding='utf-8') as f2:\n",
    "                    f2.write(doc)\n",
    "    print('文件存储完毕')\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6、提取训练集所需的数据\n",
    "* 随机抽取10个文件作为训练源数据\n",
    "* 抽取2个不同于训练集的数据作为测试源数据存放入test_choice中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件存储完毕\n",
      "['news.allsites.380806.txt', 'news.allsites.680806.txt', 'news.allsites.1510806.txt', 'news.allsites.630806.txt', 'news.allsites.110806.txt', 'news.allsites.330806.txt', 'news.allsites.510806.txt', 'news.allsites.1120806.txt', 'news.allsites.080806.txt', 'news.allsites.180806.txt']\n"
     ]
    }
   ],
   "source": [
    "choice_dir = './train_choice'\n",
    "file_list = choice_files(half_dir,choice_dir,10)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7、提取文档文本内容\n",
    "* 遍历预处理好的xml文件，提取content标签中的文本并重新保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_read(file_dir,labels,path):\n",
    "    # 遍历获取file_dir文件夹下的文件夹和文件目录\n",
    "    for _,_,files in os.walk(file_dir):\n",
    "        for f in files:\n",
    "            filename = file_dir+'/'+f\n",
    "            # js标签处理\n",
    "            doc = minidom.parse(filename)\n",
    "            root = doc.documentElement\n",
    "            claimtext = root.getElementsByTagName('content')\n",
    "            claimurl = root.getElementsByTagName('url')\n",
    "            for ind in range(len(claimurl)):\n",
    "                if claimtext[ind].firstChild == None:\n",
    "                    continue\n",
    "                # 获取url对象\n",
    "                url = urlparse(claimurl[ind].firstChild.data)\n",
    "                # 获取url中体现文章类型的关键字符串\n",
    "                url_lb = url.hostname.strip().split('.')[0]\n",
    "                # 判断url_lb对应的文章类型\n",
    "                if check_class(url_lb,labels):\n",
    "                    # 自动创建文件夹path（存放所有本阶段处理后的文件）\n",
    "                    if not os.path.exists(path):\n",
    "                        os.makedirs(path)\n",
    "                    # 在path路径下自动创建文章类别文件夹\n",
    "                    if not os.path.exists(path+'./'+labels[url_lb]):\n",
    "                        os.makedirs(path+'./'+labels[url_lb])\n",
    "                    file_name = path+'./'+labels[url_lb]+'./'+\"{}.txt\"\\\n",
    "                    .format(labels[url_lb])\n",
    "                    # 在每一个分类文件夹下创建处理后的文本文件\n",
    "                    with open(file_name,\"a+\",encoding='utf-8') as file_in:\n",
    "                        file_in.write(claimtext[ind].firstChild.data+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义存放最后文件的文件夹地址\n",
    "path = \"./train_after\"\n",
    "# 将选好的文件进行纯文本提取和分类存储\n",
    "file_dir = 'train_choice'\n",
    "file_read(file_dir,labels,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将选好的文件进行纯文本提取和分类存储\n",
    "file_dir = 'test_choice'\n",
    "# 定义存放最后文件的文件夹地址\n",
    "test_path = \"./test_after\"\n",
    "\n",
    "file_read(file_dir,labels,test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8、预处理数据小结\n",
    "* 到该阶段已基本获取所需要的纯文本文件，走完了万里长征第一步\n",
    "* 文本预处理阶段主要是细节问题比较多包括以下方面：\n",
    "* 1、xml标签的处理\n",
    "* 2、文本的编码格式要注意\n",
    "* 3、os.walk和os.makedirs的使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、数据清洗\n",
    "* 在数据预处理基础之上进一步对数据格式进行清洗\n",
    "* 包括删除特殊字符，去除标点和中文分词等工作\n",
    "* 需要自定义函数，并使用相关工具包如jieba等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、提取文本中的汉字数据\n",
    "* 此处可以直接通过提取文章中的汉字方式过滤数字和特殊符号\n",
    "* 去除中文停用词\n",
    "###### 有些项目可能会专门去除数据中的url及无效数字和标点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入字符串匹配需要的工具包\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取停用词列表\n",
    "with open('files/stopwords_1208.txt','r',\n",
    "          encoding='utf-8-sig') as f:\n",
    "   stopwords = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 识别是否存在数字或字符\n",
    "# def hasNumChar(strs):\n",
    "#     return bool(re.search(r'\\d', strs))\n",
    "\n",
    "# # 识别是否是字母\n",
    "# def isChar(strs):\n",
    "#     return bool(re.search(r'[a-zA-Z]', strs))\n",
    "\n",
    "# # 识别是否含有特殊字符和标点\n",
    "# def hasSymbol(strs):\n",
    "#     pat = r'''[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，［“ ”。］：‘ ’？、~@#￥%……&*（）．]'''\n",
    "#     return bool(re.search(pat, strs))\n",
    "\n",
    "# 判断词语是否为汉字\n",
    "def isHans(strs):\n",
    "    pat = r'[\\u4e00-\\u9fa5]+'\n",
    "    return bool(re.match(pat, strs))\n",
    "\n",
    "# 判断分词的基本特征\n",
    "def check(word):\n",
    "    word = word.strip().replace(' ','')\n",
    "    if isHans(word):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 综合处理\n",
    "def preprocessing(sens):\n",
    "    res = []\n",
    "    for word in sens:\n",
    "        if check(word):\n",
    "            res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、获取训练集及测试集的分类数据\n",
    "* 为使项目结构更加清晰可以建立多层文件夹路径\n",
    "* 主要利用os.walk和os.makedirs等方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取处理文件数据\n",
    "def handle_files(file_path,new_path):\n",
    "    # 创建new_path存储处理好后的文件\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "    # 进入预处理后的主文件夹sougou_after\n",
    "    for _,dirs,_ in os.walk(file_path):\n",
    "        print(dirs)\n",
    "        for d in dirs:\n",
    "            d_path = file_path+'./'+d\n",
    "            new_d_path = new_path+'./'+d\n",
    "            # 在new_path文件夹中穿件各种分类文件夹\n",
    "            if not os.path.exists(new_d_path):\n",
    "                os.makedirs(new_d_path)\n",
    "            # 进入各种分类文件夹\n",
    "            for _,_,files in os.walk(d_path):\n",
    "                for file in files:\n",
    "                    d_p_file = d_path+'./'+file\n",
    "                    n_d_file = new_d_path+'./'+file\n",
    "#                     print('------------')\n",
    "#                     print(d_p_file)\n",
    "                    # 打开分类文件夹中的文件\n",
    "                    with open(d_p_file,'r',encoding='utf-8') as f1:\n",
    "                        docs = f1.readlines()\n",
    "#                         print('文件读取完毕')\n",
    "                        # 创建一个空列表收集处理后的句子\n",
    "                        res_docs = []\n",
    "                        # 读取每一行的数据\n",
    "                        for doc in docs:\n",
    "                            # 将每行的数据进行分词\n",
    "                            sens = jieba.cut(doc)\n",
    "                            # 提取文本中的汉语数据\n",
    "                            res = preprocessing(sens)\n",
    "                            result = ' '.join(res)\n",
    "                            result += '\\n'\n",
    "                            res_docs.append(result)\n",
    "                        print(len(res_docs))\n",
    "                        # 将处理好后的res_docs里的句子写入新的文件夹\n",
    "                        with open(n_d_file,'a+',\n",
    "                                  encoding='utf-8') as f2:\n",
    "                            for docum in res_docs:\n",
    "                                f2.write(docum)\n",
    "                            print('文件写入小结')\n",
    "    print('文章处理完毕')                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '健康', '军事', '奥运', '女人', '房产', '教育', '文化', '汽车', '科技', '财经']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\Python\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.302 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9699\n",
      "文件写入小结\n",
      "359\n",
      "文件写入小结\n",
      "670\n",
      "文件写入小结\n",
      "2772\n",
      "文件写入小结\n",
      "3342\n",
      "文件写入小结\n",
      "558\n",
      "文件写入小结\n",
      "2585\n",
      "文件写入小结\n",
      "369\n",
      "文件写入小结\n",
      "2833\n",
      "文件写入小结\n",
      "811\n",
      "文件写入小结\n",
      "12001\n",
      "文件写入小结\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "文章处理完毕\n"
     ]
    }
   ],
   "source": [
    "# 训练文本数据格式化\n",
    "# test_path = './sougoo'\n",
    "train_new_path = 'train_final'\n",
    "handle_files(path,train_new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '健康', '军事', '奥运', '女人', '房产', '教育', '文化', '汽车', '科技', '财经']\n",
      "965\n",
      "文件写入小结\n",
      "42\n",
      "文件写入小结\n",
      "65\n",
      "文件写入小结\n",
      "274\n",
      "文件写入小结\n",
      "322\n",
      "文件写入小结\n",
      "73\n",
      "文件写入小结\n",
      "235\n",
      "文件写入小结\n",
      "38\n",
      "文件写入小结\n",
      "278\n",
      "文件写入小结\n",
      "87\n",
      "文件写入小结\n",
      "1193\n",
      "文件写入小结\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "文章处理完毕\n"
     ]
    }
   ],
   "source": [
    "# 测试文本数据格式化\n",
    "test_new_path = 'test_final'\n",
    "handle_files(test_path,test_new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3、列表化训练集和测试集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义测试数据矩阵函数\n",
    "def label_cont2list(data_path):\n",
    "    # 定义根目录内容和标签列表\n",
    "    dir_cont_list,dir_label_list = [],[]\n",
    "    for _,dirs,_ in os.walk(data_path):\n",
    "        for d in dirs:\n",
    "            # 进入分类的子文件夹\n",
    "            d_dir = data_path+'/'+d\n",
    "            for _,_,files in os.walk(d_dir):\n",
    "                # 定义子文件夹内容列表\n",
    "                d_cont_list = []\n",
    "                for file in files:\n",
    "                    file_name = d_dir+'/'+file\n",
    "                    # 读取分类子文件夹内的文件\n",
    "                    with open(file_name,encoding='utf-8') as f:\n",
    "                        # 将文件内的每行数据作为元素存入列表\n",
    "                        d_c_list = [k.strip() for k in f.readlines()]\n",
    "                    d_cont_list.extend(d_c_list)\n",
    "            # 分类文件夹级别内容列表合并\n",
    "            dir_cont_list.extend(d_cont_list)\n",
    "            # 将每个子文件夹的所有标签列表元素进行统一化后合并\n",
    "            dir_label_list.extend(\n",
    "                [str(d) for _ in range(len(d_cont_list))])\n",
    "    return dir_label_list,dir_cont_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取训练数据集的标签列表和内容列表\n",
    "train_path = 'train_final'\n",
    "train_label_list,train_cont_list = label_cont2list(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取测试集的标签列表和内容列表\n",
    "test_path = 'test_final'\n",
    "test_label_list,test_cont_list = label_cont2list(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35999\n",
      "['期 的 中奖号码 是 从 大小 上 分析 本期 开出 组合 本期 小数 仍 占优势 但 本期 基本 恢复 平稳 预计 下期 会 调整 平稳 出现 提示 下期 关注 或 从 奇偶 上 分析 本期 奇偶比 为 本期 奇数 略有 优势 本期 奇偶 数 做出 调整 估计 下期 可能 会 平稳 出现 提示 下期 关注 或 的 组合 从和值 上 分析 本期 和 值 开出 本期 和 值 点位 较 上期 有所 回升 近期 和 值 点位 波动 较大 预计 下期 可能 还会 出现 小幅 上升 估计 会 在 适中 点位 出现 下期 关注 之间 从 遗漏 总值 上 分析 本期 遗漏 总值 为 本期 遗漏 总值 仍 较 低 开出 号码 多以 重码 开出 预计 下期 可能 会 平稳 出现 下期 关注 之间', '新浪 体育讯 北京 时间 月 日 凌晨 国际田联 大奖赛 捷克 奥斯特 拉瓦 站 的 比赛 陆续 展开 在 男子 米 的 比赛 中 刚刚 打破 了 男子 百米 世界纪录 的 牙买加 小将 博 尔特 以 秒 的 成绩 创造 了 米 项目 的 赛季 最好 成绩 网页 不 支持', '您 所在 的 位置 腾讯 首页 体育频道 图片 滚动 图集 正文 第 页 图 腾讯 体育讯 北京 时间 月 日 英国女王 草地 杯 第二轮 比赛 全面 展开 赛会 头号 种子 纳达尔 以 和 击败 持 外卡 参赛 的 瑞典 老将 比约克 曼 轻松 斩获 本年度 草地 赛季 的 首场 胜利', '第 页 图 得分 后卫 杰里 韦斯特 他 的 绰号 叫作 关键球 先生 他 的 能力 无需 用 语言 来 描述 只 需要 注意 这样 一个 事实 在 这支 湖人队 史 最强 阵容 里 科比 不得不 被 他 挤 到 了 小 前锋 位置 上 分类 信息 企业 服务 招商 信息 热点 信息 热门 推荐 体育', '英超 新 赛季 赛程 已经 公布 曼联 首轮 将 主场 出战 纽卡 切尔西 主场 战 朴茨茅斯 阿森纳 主场 对 西布朗 利物浦 则 需 作客 桑德兰 网易 体育 月 日 消息 北京 时间 周一 傍晚 点 赛季 英超 赛程 正式 对外 公布 英超 新 赛季 将 从 月 日 的 社区 盾 开始 对阵 双方 是 上赛季 英超 冠军 曼联 和 足总杯 冠军 朴茨茅斯 随后 一周 在 月 日 英超 首轮 将 陆续 开战 卫冕冠军 曼联 首轮 将 主场 出战 纽卡斯尔 切尔西 和 阿森纳 也 是 主场 分别 对 朴茨茅斯 和 西布朗 只有 利物浦 需要 作客 桑德兰 点击 查看 赛季 详细 赛程 英超 首轮 对阵 月 日 曼联 纽卡斯尔 切尔西 朴茨茅斯 阿森纳 西布朗 桑德兰 利物浦 博尔顿 斯托克 城 维拉 曼城 米堡 热刺 西汉姆 维甘 曼联 九月 激战 曼联 新 赛季 揭幕战 和 最后 一轮 分别 为 主客场 在 月 日 对 纽卡后 弗格森 的 球队 在 次年 月 日 将 以 作客 升班马 胡尔城 来 结束 整个 赛季 曼联 的 艰苦 赛程 在 月 就 将 开始 他们 将 在 连续 两个 周末 遭遇 利物浦 和 切尔西 其中 月 日 作客 安菲尔德 日 作客 斯坦福桥 红魔 首 循环 对阵 阿森纳 是 在 月 日 同样 是 客场 月 日 曼联 将 回到 主场 对战 切尔西 月 日 主场 打 利物浦 月 日 阿森纳 会 作客 老 特拉福德 挑战 曼联 那 也 是 英超 倒数 第二轮 切尔西 首 循环 对 三强 均 为 主场 切尔西 新帅 斯科拉里 的 第一个 挑战 将 是 主场 对 足总杯 冠军 朴茨茅斯 在 四强 对阵 方面 切尔西 除了 月 主场 打 曼联 还 将 在 月 日 主场 对 利物浦 月 日 主场 对 阿森纳 月 日 蓝军 将 回访 利物浦 月 日 作客 阿森纳 赛季 最后 一轮 则 作客 桑德兰 阿森纳 利物浦 赛程 枪手 阿森纳 的 开局 不 困难 月 日 他们 的 对手 是 升班马 西布朗 月 日 的 赛季 最后 一轮 阿森纳 则 将 在 主场 出战 另 一支 升班马 斯托克 城 阿森纳 的 大战 除了 前面 提到 的 与 曼联 和 切尔西 之战外 月 日 需要 主场 对 利物浦 作客 安菲尔德 则 在 月 日 利物浦 赛季 首轮 对手 是 桑德兰 客场 最后 一轮 则 是 主场 对 热 刺 本文 来源 网易 体育 作者 国际足球 专题 策划 更 多 策划 体育 今日热点 国奥 敲定 奥运 名单 全部 为 杜伊 指定 球员 中国 奥运 夺金 选手 最少 奖励 百万 罗伯斯 再 跑 秒 施压 刘翔 科比 竟 欢迎 中国 球迷 狂嘘 梦 八 基德 力压 保罗 成梦八先发 详细', '飞人 乔丹 影响 了 一代人 是 他 使 许多 中国 人 知道 了 什么 是 是 他 让 人们 了解 到 原来 篮球 也 可以 打 得 那么 飘逸 那么 让 我们 测下 你 对 乔丹 了解 多少 体育', '在 与 卡特 同个 赛季 进入 联盟 后 昔日 的 北卡 双壁 之一 贾米森 就 一直 被 当成 与 卡特 比较 的 对象 虽然 始终 因为 较为 偏向 蓝领 的 风格 和平 实 的 职业生涯 而 使得 他 在 个人 评价 方面 与 卡特 相距甚远 但 这 并 不 妨碍 贾米森 成为 一位 一流 的 内线 球星 在 大学 生涯 的 三年 里 贾米森 场均 能够 得到 分 和 个 篮板 最终 在 年 的 选秀 大会 首轮 第四位 被 猛龙队 选中 不过 他 随后 就 被 当成 交易 卡特 的 筹码 赛季 末 入选 最佳 新人 第二队 时过境迁 如今 的 贾米森 也 成为 季后赛 的 常客 和 全明星 级别 的 球员 即使 将 他 与 卡特 比肩 应该 也 不会 有人 过于 厚此薄彼', '腾讯 体育讯 北京 时间 月 日 年 选秀 大会 将 于本周 五在 纽约 麦迪逊 花园 广场 举行 火箭队 今年 的 目标 是 什么 总经理 达 瑞尔 莫雷 的 选秀 策略 是 什么 在 大个子 新秀 方面 我们 有 一些 机会 我们 的 大个子 也 经常出现 伤病 情况 莫雷 如是说 莫雷言 有 所指 姚明 近几年 伤病 不断 迪肯 贝 穆托姆博 日渐 老迈 所以 火箭队 选秀 的 当务之急 是 为 中锋 姚明 找 一个 备胎 三天 后 的 选秀 大会 上 精于 数据 的 莫雷有 哪些 大个子 新秀 可以 挑选 呢 我们 将 为 您 列举 火箭 可能 觊觎 的 前锋 或者 中锋 图 罗伊 希尔伯特 乔治敦大学 希尔伯特 坚持 读完 了 大学 四年 他 所在 的 乔治敦大学 有着 优良 的 中锋 传承 帕特里克 尤因 阿伦 佐 莫宁和迪 肯贝 穆托姆博 都 出自于 此 米 的 身高 和 出色 的 防守 让 希尔伯特 在 中锋 位置 行情看涨 他 很 有 可能 在 火箭 挑选 之前 被 人 截 和', '本报讯 记者 孙卫 中国 职业 斯诺克 巡回赛 第二站 分区赛 西安站 昨天 收杆 经过 三天 的 激烈 争夺 三位 西安 本土 选手 杨旭 马少君 张衍武 闯入 四强 获得 了 中 巡赛 正赛 阶段 比赛 资格 中国 职业 斯诺克 巡回赛 今年 进行 了 赛制 改革 推出 了 正赛 资格赛 分区赛 三级 一体 的 竞赛 模式 第一站 分区赛 已经 结束 第二站 分区赛 依次 在 北京 西安 上海 惠州 四地 举行 西安站 比赛 是 我市 近年来 举办 的 最高 规格 的 斯诺克 赛事 共有 多位 来自 全国 的 高手 参加 在 比赛 中 西安 本土 选手 表现出色 杨旭 马少君 张衍 武均 以 较大 优势 战胜 各自 对手 顺利 挺进 四强 占据 另 一四强 席位 的 是 南京 选手 陈文 渭南 选手 张伟 和 他 苦战 局 最终 以 惜败 这四名 选手 将 参加 于 月 在 深圳 举行 的 中 巡赛 正赛 我市 名将 杨 擎天 将 以 较 高 的 排名 直接 入围 正赛', '瑞士 的 转播 同行 对 全世界 开 了 一笑 玩笑 当 穆图 走上 码线 时 电视屏幕 上 出现 的 却是 辣妹 和 帅哥 然后 将 画面 切回 赛场 布冯 立功 了 穆图 本 可以 借着 梅开二度 成为 英雄 但 曾经 同在 尤文 效力 让布 冯对 穆图 的 射门 角度 有 了 先天 的 条件反射 如果 这个 点球 换 一个 人 主罚 或许 结果 就是 另 一种 色彩 当然 不能 责怪 穆图 他 做 得 已经 足够 出色 整支 罗马尼亚 队 都 应该 得到 人们 的 尊重 他们 在 死亡 之组 中 亦步亦趋 却渐 行渐 稳 就 像 暗藏杀机 的 一颗 定时炸弹 谁 都 不能 忽视 他们 的 爆破 力 否则 就 会 被 炸 得 粉身碎骨 连续 逼平 世界杯 冠亚军 意大利 和 法国 更 早 之前 他们 更是 在 预选赛 中力 压 本届 欧洲杯 首轮 的 最佳 球队 荷兰 这 一切 都 不是 一种 偶然 第二轮 过后 我们 恍然 发现 本届 杯赛 许多 逝去 的 强队 正在 重新 崛起 比如 昨天 的 克罗地亚 今天 的 罗马尼亚 但 对手 的 复苏 不能 成为 意大利 沉沦 的 全部 借口 多 纳多尼 手下 的 这支 意大利 和 概念 中 的 传统 蓝色 基调 很 不 一样 尤其 体现 在 防守 上 更为 明显 是 个 球迷 都 知道 意大利 每逢 大赛 出彩 防守 至上 是 根本 理念 在 意大利 球员 的 踢球 哲学 中 防守 已 深入骨髓 这里 我 不想 怀疑 这批 意大利 球员 的 防守 态度 只是 防守 有时 并 不是 态度 所 能 决定 的 能力 两年 前 里 皮 的 国家队 在 世界杯 场 比赛 丢掉 球 现在 多 纳多尼 的 球队 在 分钟 的 比赛 中 已经 丢失 球 如果 不是 荷兰 和 罗马尼亚 门前 错失 三两 契机 这个 数字 很 有 可能 继续 扩大 赛前 折 了 卡纳瓦罗 只是 意大利 防守 不力 的 表象 本土 后防 球员 的 青黄不接 才 是 根本 顽疾 让 打惯 了 右边后卫 的 帕努奇 出任 中卫 结果 造成 两个 致命 失误 一个 失球 让 在 巴萨 状态 平平的 赞 布罗 塔 首发 根本 看不到 他 往常 凌厉 的 冲刺 和 像 弹簧 一样 收缩自如 的 来回 奔跑 而 神奇 的 格罗索 最后 关头 再次 倒 在 了 禁区 内 但 这 一次 裁判 没有 判罚 点球 比赛 中 他 慢动作 一样 的 过 人 很 难 重复 当年 的 神采 看看 意大利 核心 后防 的 年龄 构成 吧 帕努奇 岁 赞 布罗 塔 岁 格罗索 岁 还有 坐在 板凳 席上 腿 痒痒 的 马特拉 济 和 卡纳瓦罗 也 都 是 岁 年轻 的 似乎 只 剩下 了 巴尔扎 利 和 基耶 利尼 两人 分别 只有 岁 和 岁 但 巴尔扎 利早 在 里 皮 时代 就 已 被 证明 不堪 大 用 本届 欧洲杯 第一场 首发 再度 引起 巨大 争议 基耶 利尼是 上赛季 尤文 后防 涌现 的 新星 他 米 的 身材 猛然 让 人 想起 内斯塔 一样 的 强健 一样 的 勇猛 只是 经验 难免 不足 实力 有待 检验 仰仗 这样 的 后防 意大利 怎能 继续 世界杯 的 神奇 多 纳多尼 似乎 早有 预感 首场 比赛 他 保守 地 派出 米兰 三 后腰 就是 希望 能 借此 加固 后防 前 的 屏障 实现 万无一失 的 保驾护航 但 荷兰 神出鬼没 的 身后 球 将 意大利 闻名世界 的 混 泥土 防线 冲得 七零八落 更是 让 米兰 三人组 的 默契 变成 了 北京 人常 说 的 墨迹 打 罗马尼亚 多 纳多尼 被迫 换阵 连带 积分 形势 他 将 罗马 双星 排入 首发 但 这么一来 罗马尼亚 在 中路 获得 更 多 的 得球 空间 两个 边路 因此 得益 他们 多次 利用 中路 向 边路 的 转移 形成 两侧 的 攻击 波 意大利 后防 沦陷 过去 的 元老 们 似乎 也 急 了 他们 说 现在 的 意大利 后卫 不 懂得 盯人 只会 联防 这个 道理 说 得 不错 别看 贝尔 戈米 费拉拉 他们 只会 跟随 人 的 影子 跑 这 其实 也 是 一门 高深 的 学问 丢 了 铁桶阵 意大利 的 进攻 也 无法 展开 拳脚 具有 历史 韵味 的 防反 俨然 不见 反而 在 首场 比赛 被 荷兰 复制 自家 秘笈 没有 历史 传统 打法 多 纳多尼 只能 寄望 多尼 在 前场 猛撞 猛打 冲开 血路 事实上 这个 大个子 也 很 努力 他 的 制高点 优势 帮助 德罗西 皮耶罗 等 人 赢得 绝佳 良机 可惜 门前 感觉 尚未 到家 连连 错失良机 意大利 的 进攻 打得 很 简单 只是 一味 从 边路 起球 找托尼 这种 单一型 打法 却 收到 不错 效果 只是 遇到 强队 时 这 一招 恐怕 难以 制造 打压 作用 但 我 很 同意 多 纳多尼 启用 皮耶罗 老 队长 在 意甲 荣膺 金靴 绝对 不是 偶然 两场 比赛 他 已经 有 了 五次 射门 欲望 的 增加 有望 带来 迟到 的 进球 另外 在 下一场 生死战 中 强烈建议 多 纳多尼 放弃 卡 莫拉 内西 给 卡萨诺 更 多 机会 和 皮耶罗 联袂 两边 如今 只有 增加 突破 托尼才 有 更 多 乱 中 抢点 的 机会 而 在 突破 环节 卡 莫拉 内西 的 功力 显然 已经 不如 一年 前 穆图 给 了 意大利 生还 的 机会 可是 最后 一战 对 法国 岂 是 那么 容易 可以 拿下 法国 善 打 反击 以 意大利 这 条 后防 的 能力 十有八九 危在旦夕 阿凯']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_cont_list))\n",
    "# print(dir_cont_list)\n",
    "print(train_cont_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、生成词汇表\n",
    "* 收集训练集文本内容列表词作为词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入词汇收集工具包\n",
    "from collections import Counter\n",
    "\n",
    "# 定义获取词汇表的函数\n",
    "def getVocabList(content_list, vocab_size):\n",
    "    str_allContent = ''.join(content_list)\n",
    "    counter = Counter(str_allContent)\n",
    "    vocab_list =[k[0] for k in counter.most_common(vocab_size)]\n",
    "    return vocab_list\n",
    "\n",
    "# 生成词汇表文件\n",
    "def generateVocabList(content_list, vocab_size):\n",
    "    vocab_list = getVocabList(content_list, vocab_size)\n",
    "    print(vocab_list[:10])\n",
    "    with open('files/vocab_list.txt', 'w+', encoding='utf-8') as f:\n",
    "        for vocab in vocab_list:\n",
    "            f.write(vocab+'\\n')\n",
    "        print('词汇表创建完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '的', '一', '在', '是', '中', '国', '有', '不', '了']\n",
      "词汇表创建完成\n"
     ]
    }
   ],
   "source": [
    "generateVocabList(train_cont_list, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5、词汇表词向量化\n",
    "* 利用词汇表文件将数据进行词向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取词汇表文件数据，存入列表中\n",
    "with open('files/vocab_list.txt',encoding='utf-8') as f:\n",
    "    vocab_list = [k.strip() for k in f.readlines()]\n",
    "\n",
    "# 将词汇列表中的数据转化为键为数据，值为序号的字词汇典\n",
    "word2id_dict = dict([(b,a) for a,b in enumerate(vocab_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义词向量化函数，将数据转化为词向量\n",
    "def list2vector(word2id_dict,train_cont_list):\n",
    "    # 定义一个词汇字典与文本内容的映射关系\n",
    "    cont2id_list = lambda content:[\n",
    "        word2id_dict[\n",
    "            word] for word in content if word in word2id_dict]\n",
    "    # 将训练集数据映射为词汇字典对应的词向量\n",
    "    train_idlist_list = [cont2id_list(\n",
    "        content) for content in train_cont_list]\n",
    "    return train_idlist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、模型的建立\n",
    "* 建立CNN模型进行数据训练和预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、导入数据包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关数据包\n",
    "import tensorflow as tf \n",
    "import tensorflow.contrib.keras as kr \n",
    "from sklearn.preprocessing import LabelEncoder as spl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、固定参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定参数初始化\n",
    "vocab_size = 5000 # 词汇表的大小\n",
    "seq_length = 600 # 序列长度\n",
    "embedding_dim = 64 # 词向量维度\n",
    "num_classes = 11 # 文章的类别\n",
    "num_filters = 256 # 卷积核数目\n",
    "kernel_size = 5 # 卷积核尺度\n",
    "hidden_dim = 128 # 全连接层神经元数量\n",
    "drop_keep_prob = 0.5 # dropout保留比例\n",
    "learning_rate = 0.001 # 学习率\n",
    "batch_size = 64 # 每批次训练样本大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、样本集合格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练样本映射的词向量矩阵\n",
    "train_idlist_list = list2vector(\n",
    "    word2id_dict,train_cont_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62, 1, 5, 601, 373, 742, 4, 151, 11, 134, 10, 38, 592, 46, 62, 77, 27, 254, 106, 46, 62, 134, 116, 502, 556, 444, 239, 118, 46, 62, 91, 46, 1018, 446, 162, 551, 234, 148, 40, 62, 16, 199, 242, 162, 551, 27, 56, 167, 190, 40, 62, 102, 308, 355, 151, 579, 1317, 10, 38, 592, 46, 62, 579, 1317, 55, 13, 46, 62, 579, 116, 578, 7, 444, 239, 46, 62, 579, 1317, 116, 350, 27, 199, 242, 633, 148, 40, 62, 58, 36, 16, 162, 551, 27, 56, 167, 190, 40, 62, 102, 308, 355, 1, 254, 106, 151, 20, 245, 10, 38, 592, 46, 62, 20, 245, 77, 27, 46, 62, 20, 245, 66, 124, 251, 10, 62, 7, 109, 240, 301, 250, 62, 20, 245, 66, 124, 506, 72, 251, 11, 234, 148, 40, 62, 58, 36, 122, 16, 27, 56, 134, 302, 10, 301, 633, 148, 16, 3, 647, 5, 66, 124, 27, 56, 40, 62, 102, 308, 110, 126, 151, 1130, 1459, 192, 245, 10, 38, 592, 46, 62, 1130, 1459, 192, 245, 13, 46, 62, 1130, 1459, 192, 245, 502, 251, 261, 77, 27, 373, 742, 64, 28, 99, 742, 77, 27, 234, 148, 40, 62, 58, 36, 16, 162, 551, 27, 56, 40, 62, 102, 308, 110, 126], [54, 528, 86, 321, 341, 173, 207, 25, 126, 41, 24, 1277, 1099, 6, 277, 837, 218, 11, 601, 51, 1086, 284, 127, 183, 171, 328, 1092, 521, 1, 55, 51, 861, 255, 211, 77, 3, 480, 187, 451, 1, 55, 51, 5, 710, 710, 287, 503, 9, 480, 187, 558, 451, 309, 351, 712, 619, 1, 649, 408, 97, 134, 63, 688, 182, 171, 28, 1108, 1, 33, 524, 452, 414, 9, 451, 357, 125, 1, 51, 490, 70, 144, 33, 524, 113, 518, 8, 293, 130], [754, 109, 3, 1, 124, 403, 581, 341, 291, 518, 86, 321, 618, 185, 327, 593, 1374, 72, 327, 232, 194, 184, 105, 518, 327, 581, 341, 86, 321, 341, 173, 207, 25, 126, 41, 24, 445, 6, 247, 438, 1232, 50, 568, 105, 229, 511, 55, 51, 90, 100, 211, 77, 51, 16, 349, 373, 227, 187, 584, 221, 182, 28, 20, 394, 863, 130, 128, 493, 271, 51, 1, 644, 878, 415, 63, 55, 386, 284, 986, 671, 865, 2201, 435, 46, 14, 104, 1232, 50, 51, 490, 1, 291, 29, 425, 84], [105, 518, 327, 81, 38, 39, 714, 1095, 212, 1767, 183, 171, 53, 1, 2764, 373, 1209, 67, 102, 782, 75, 306, 61, 53, 1, 36, 69, 222, 323, 94, 658, 605, 34, 1226, 636, 220, 323, 48, 308, 178, 23, 243, 2, 17, 131, 117, 3, 23, 293, 667, 12, 79, 635, 70, 202, 800, 329, 212, 248, 55, 8, 81, 8, 282, 53, 1761, 35, 9, 134, 45, 912, 124, 403, 10, 38, 479, 156, 219, 210, 26, 393, 181, 580, 189, 156, 219, 356, 66, 156, 219, 356, 267, 381, 905, 86, 321], [445, 390, 54, 51, 490, 51, 334, 142, 42, 15, 274, 986, 218, 291, 511, 63, 89, 29, 27, 209, 1152, 493, 655, 182, 215, 89, 29, 209, 2114, 1406, 1972, 183, 585, 908, 584, 89, 29, 30, 215, 274, 1072, 84, 396, 1376, 410, 323, 67, 455, 1491, 275, 485, 113, 169, 86, 321, 41, 24, 354, 219, 173, 207, 25, 126, 216, 2, 2808, 827, 66, 51, 490, 445, 390, 51, 334, 194, 231, 30, 128, 15, 274, 445, 390, 54, 51, 490, 63, 151, 41, 24, 1, 469, 186, 1385, 77, 385, 30, 800, 496, 73, 4, 10, 51, 490, 445, 390, 542, 314, 986, 218, 20, 342, 192, 568, 542, 314, 2114, 1406, 1972, 183, 488, 39, 2, 216, 3, 41, 24, 445, 390, 291, 511, 63, 861, 255, 77, 209, 714, 1830, 542, 314, 986, 218, 291, 511, 63, 89, 29, 27, 209, 1152, 493, 183, 182, 655, 182, 215, 20, 585, 908, 584, 59, 4, 89, 29, 38, 316, 30, 2114, 1406, 1972, 183, 20, 215, 274, 1072, 220, 7, 84, 396, 1376, 323, 48, 67, 455, 1491, 275, 485, 66, 394, 489, 155, 51, 490, 637, 540, 51, 334, 445, 390, 291, 511, 30, 800, 41, 24, 986, 218, 1152, 493, 183, 182, 655, 182, 215, 2114, 1406, 1972, 183, 585, 908, 584, 215, 274, 1072, 1491, 275, 485, 84, 396, 1376, 688, 182, 1094, 183, 662, 284, 434, 421, 328, 986, 434, 451, 1782, 356, 1135, 215, 833, 1003, 421, 1349, 986, 218, 925, 41, 675, 209, 986, 218, 54, 51, 490, 1223, 1002, 209, 20, 70, 39, 2, 511, 38, 316, 13, 89, 455, 29, 3, 41, 24, 30, 1152, 493, 39, 1270, 132, 908, 1, 75, 79, 3, 107, 14, 41, 24, 63, 28, 67, 455, 301, 646, 371, 1166, 182, 434, 34, 272, 763, 242, 17, 51, 490, 986, 218, 1, 1474, 1136, 51, 334, 3, 41, 83, 63, 77, 385, 53, 60, 63, 3, 413, 255, 176, 17, 216, 1047, 1036, 847, 84, 396, 1376, 20, 655, 182, 215, 95, 5, 41, 24, 67, 455, 280, 1087, 182, 275, 24, 67, 455, 183, 1306, 632, 1370, 532, 1597, 291, 1543, 504, 30, 800, 585, 908, 584, 4, 3, 41, 24, 101, 243, 4, 455, 29, 41, 24, 986, 218, 63, 240, 35, 89, 29, 30, 209, 655, 182, 215, 41, 24, 89, 29, 287, 84, 396, 1376, 41, 24, 585, 908, 584, 16, 67, 455, 415, 171, 328, 632, 275, 1106, 209, 986, 218, 290, 59, 4, 445, 390, 987, 116, 105, 229, 511, 655, 182, 215, 291, 1543, 504, 30, 165, 202, 431, 13, 89, 29, 655, 182, 215, 54, 1058, 183, 248, 328, 212, 1, 105, 2, 17, 1106, 209, 63, 4, 89, 29, 30, 342, 192, 568, 542, 314, 2114, 1406, 1972, 183, 3, 296, 202, 30, 800, 73, 100, 655, 182, 215, 546, 9, 41, 89, 29, 287, 986, 218, 122, 63, 3, 41, 24, 89, 29, 30, 84, 396, 1376, 41, 24, 89, 29, 30, 585, 908, 584, 41, 24, 1088, 314, 63, 240, 825, 84, 396, 1376, 41, 24, 67, 455, 585, 908, 584, 51, 490, 70, 39, 2, 511, 410, 67, 455, 1491, 275, 485, 585, 908, 584, 84, 396, 1376, 51, 334, 1611, 136, 585, 908, 584, 1, 77, 383, 8, 1000, 405, 41, 24, 53, 60, 1, 30, 136, 4, 301, 646, 371, 215, 274, 1072, 41, 24, 1, 51, 490, 70, 39, 2, 511, 585, 908, 584, 410, 63, 3, 89, 29, 27, 209, 613, 2, 293, 301, 646, 371, 183, 662, 284, 434, 585, 908, 584, 1, 11, 209, 546, 9, 45, 100, 167, 35, 1, 114, 986, 218, 20, 655, 182, 215, 110, 209, 128, 41, 24, 323, 48, 89, 29, 30, 84, 396, 1376, 67, 455, 280, 1087, 182, 275, 410, 3, 41, 24, 84, 396, 1376, 51, 490, 291, 511, 30, 136, 4, 1491, 275, 485, 455, 29, 70, 39, 2, 511, 410, 4, 89, 29, 30, 356, 1135, 46, 184, 34, 332, 113, 169, 86, 321, 67, 57, 6, 277, 342, 75, 377, 228, 423, 527, 177, 64, 423, 527, 86, 321, 168, 24, 356, 66, 6, 127, 1814, 93, 127, 92, 164, 364, 90, 108, 13, 978, 630, 197, 93, 75, 163, 5, 6, 127, 92, 772, 37, 236, 136, 70, 326, 601, 979, 558, 140, 311, 1011, 183, 338, 717, 1108, 549, 441, 566, 920, 248, 55, 982, 811, 844, 5, 6, 75, 744, 1250, 2122, 1043, 793, 91, 275, 69, 441, 160, 311, 33, 1043, 793, 306, 32, 637, 540], [693, 12, 1507, 1096, 358, 395, 9, 2, 295, 12, 4, 53, 304, 571, 64, 5, 6, 12, 402, 185, 9, 519, 289, 4, 4, 53, 279, 12, 60, 9, 319, 35, 263, 34, 798, 75, 59, 58, 28, 287, 81, 290, 289, 1899, 1618, 290, 289, 279, 31, 60, 623, 40, 283, 30, 1507, 1096, 9, 319, 64, 326, 86, 321], [3, 114, 493, 171, 101, 17, 51, 490, 71, 141, 218, 1117, 39, 2260, 24, 1, 173, 493, 496, 1857, 110, 2, 1765, 451, 908, 83, 2, 333, 282, 154, 33, 114, 493, 171, 55, 251, 1, 30, 595, 611, 159, 385, 501, 135, 13, 251, 13, 1004, 226, 1088, 436, 1, 213, 132, 20, 162, 117, 1, 668, 26, 61, 1658, 82, 304, 81, 53, 3, 17, 12, 433, 47, 73, 100, 114, 493, 171, 170, 1015, 794, 612, 118, 23, 149, 8, 2109, 1647, 1765, 451, 908, 33, 13, 2, 124, 2, 362, 1, 88, 256, 75, 572, 3, 11, 120, 61, 1658, 1, 165, 14, 212, 1765, 451, 908, 29, 431, 36, 602, 81, 35, 38, 20, 17, 798, 537, 70, 501, 3, 14, 1, 236, 928, 11, 16, 291, 511, 105, 296, 124, 282, 1472, 674, 79, 236, 5, 8, 76, 53, 488, 39, 83, 282, 154, 33, 147, 169, 493, 171, 1, 1001, 742, 51, 490, 1047, 141, 236, 70, 846, 54, 12, 105, 229, 79, 25, 76, 603, 1831, 157, 168, 1, 1765, 451, 908, 59, 33, 13, 490, 39, 51, 1, 281, 455, 20, 90, 119, 572, 339, 316, 1, 75, 163, 440, 304, 63, 53, 114, 493, 171, 55, 1679, 205, 235, 59, 8, 16, 7, 12, 76, 43, 1276, 123, 1602, 1657], [581, 341, 86, 321, 341, 173, 207, 25, 126, 41, 24, 14, 236, 928, 11, 16, 63, 43, 46, 216, 471, 3, 1152, 386, 984, 877, 1593, 665, 938, 378, 29, 552, 22, 318, 1379, 79, 168, 14, 1, 125, 345, 4, 519, 289, 192, 42, 85, 221, 644, 182, 1149, 741, 1, 236, 928, 423, 578, 4, 519, 289, 3, 11, 17, 187, 54, 928, 73, 100, 31, 60, 7, 2, 217, 96, 16, 31, 60, 1, 11, 17, 187, 59, 42, 281, 27, 56, 743, 858, 133, 348, 1149, 741, 157, 4, 112, 1149, 741, 605, 7, 109, 197, 1073, 119, 250, 420, 14, 743, 858, 8, 533, 877, 1009, 1102, 1492, 662, 1003, 688, 24, 1033, 415, 1414, 109, 28, 318, 1379, 79, 236, 928, 1, 154, 181, 110, 885, 4, 13, 5, 912, 1073, 119, 845, 2, 17, 363, 1421, 165, 111, 39, 1, 236, 928, 11, 16, 10, 472, 43, 116, 158, 1, 1149, 741, 7, 1005, 217, 11, 17, 187, 54, 928, 58, 28, 1106, 236, 898, 31, 60, 63, 13, 754, 607, 552, 318, 1379, 58, 36, 3845, 3846, 1, 45, 912, 355, 57, 5, 912, 327, 311, 630, 522, 182, 1011, 171, 1507, 570, 1486, 11, 120, 522, 182, 1011, 171, 864, 130, 910, 380, 9, 11, 120, 296, 14, 53, 109, 3, 1, 1507, 570, 1486, 11, 120, 7, 201, 444, 834, 1, 5, 912, 264, 648, 1231, 171, 212, 284, 862, 135, 585, 1016, 1946, 1149, 747, 20, 877, 1009, 1102, 1492, 662, 1003, 688, 121, 27, 80, 43, 123, 451, 1, 292, 49, 20, 27, 400, 1, 557, 757, 279, 522, 182, 1011, 171, 3, 5, 912, 124, 403, 22, 133, 155, 253, 53, 150, 7, 58, 36, 3, 318, 1379, 1106, 236, 110, 45, 282, 12, 868, 20], [46, 129, 341, 206, 57, 971, 714, 5, 6, 668, 26, 183, 797, 284, 1585, 240, 51, 105, 229, 521, 38, 186, 51, 215, 280, 521, 598, 111, 172, 1110, 42, 76, 165, 111, 1, 675, 930, 520, 772, 165, 124, 215, 280, 46, 638, 236, 136, 914, 1816, 371, 326, 1489, 365, 2031, 883, 1948, 141, 296, 202, 435, 81, 9, 5, 1585, 51, 194, 51, 848, 482, 55, 51, 19, 132, 5, 6, 668, 26, 183, 797, 284, 1585, 240, 51, 168, 14, 71, 22, 9, 51, 198, 391, 886, 381, 27, 9, 194, 51, 19, 132, 51, 38, 186, 51, 165, 339, 2, 86, 1, 721, 51, 548, 231, 105, 2, 521, 38, 186, 51, 142, 42, 272, 763, 105, 229, 521, 38, 186, 51, 608, 107, 3, 173, 207, 215, 280, 10, 195, 1040, 497, 296, 50, 552, 22, 215, 280, 521, 55, 51, 4, 31, 21, 250, 14, 34, 552, 484, 1, 70, 49, 303, 132, 1, 183, 797, 284, 51, 131, 464, 7, 64, 124, 34, 80, 90, 6, 1, 49, 136, 271, 97, 3, 55, 51, 5, 215, 280, 46, 638, 236, 136, 98, 56, 27, 400, 914, 1816, 371, 326, 1489, 365, 2031, 883, 431, 28, 251, 11, 444, 239, 209, 425, 331, 80, 30, 136, 843, 84, 1355, 71, 296, 202, 556, 158, 613, 2, 296, 202, 563, 124, 1, 4, 262, 207, 236, 136, 718, 184, 3504, 262, 236, 136, 365, 1091, 20, 53, 1136, 209, 383, 70, 501, 28, 1463, 863, 23, 296, 164, 236, 136, 63, 271, 97, 43, 41, 3, 401, 915, 552, 22, 1, 5, 1585, 51, 194, 51, 31, 21, 164, 63, 914, 1842, 111, 63, 28, 251, 49, 1, 360, 164, 333, 273, 141, 760, 194, 51], [644, 384, 1, 298, 766, 101, 22, 30, 90, 309, 351, 77, 9, 2, 1074, 1261, 1074, 154, 1492, 327, 387, 10, 742, 256, 25, 161, 514, 1734, 1002, 10, 27, 56, 1, 583, 4, 2345, 1682, 20, 1058, 1181, 159, 39, 63, 1116, 100, 655, 240, 51, 29, 274, 1397, 368, 543, 9, 1492, 327, 46, 58, 28, 889, 201, 981, 77, 229, 104, 33, 13, 445, 1172, 118, 628, 42, 101, 3, 862, 184, 432, 69, 279, 274, 1397, 30, 1492, 327, 1, 819, 267, 781, 104, 7, 9, 306, 111, 1, 576, 382, 367, 819, 157, 230, 23, 17, 66, 75, 697, 2, 17, 12, 89, 1125, 355, 571, 272, 230, 83, 4, 613, 2, 227, 400, 643, 154, 159, 8, 36, 476, 1586, 1492, 327, 53, 350, 81, 142, 42, 342, 602, 27, 400, 242, 293, 311, 371, 530, 286, 79, 121, 205, 235, 81, 35, 12, 60, 1, 1453, 99, 53, 60, 3, 895, 1508, 110, 254, 5, 1318, 389, 1318, 770, 583, 1033, 22, 1033, 551, 83, 689, 1446, 1012, 974, 96, 1, 2, 1918, 93, 25, 1793, 664, 1063, 121, 8, 36, 1594, 514, 53, 60, 1, 1171, 503, 69, 657, 410, 83, 16, 282, 1793, 81, 1314, 292, 2092, 1357, 413, 255, 1534, 162, 309, 351, 568, 542, 286, 314, 178, 11, 84, 20, 166, 6, 177, 652, 110, 45, 53, 60, 177, 4, 3, 234, 236, 51, 5, 69, 441, 46, 683, 346, 460, 568, 291, 511, 1, 70, 846, 75, 79, 875, 485, 23, 2, 655, 121, 8, 4, 2, 227, 1317, 159, 105, 229, 511, 76, 39, 31, 60, 3294, 159, 32, 56, 46, 683, 568, 51, 571, 64, 2458, 233, 1, 202, 79, 194, 3, 99, 54, 2126, 214, 55, 157, 598, 111, 1, 284, 311, 50, 286, 168, 111, 1, 311, 371, 530, 286, 118, 30, 136, 1, 446, 857, 8, 36, 33, 13, 178, 11, 84, 1308, 2565, 1, 90, 108, 889, 370, 64, 584, 64, 530, 136, 40, 1, 23, 293, 178, 11, 84, 20, 967, 813, 5, 1, 264, 322, 1088, 400, 91, 199, 150, 8, 2, 243, 862, 95, 86, 56, 3, 557, 757, 10, 177, 13, 119, 312, 4, 17, 75, 744, 121, 402, 185, 178, 11, 84, 307, 1678, 11, 51, 27, 643, 557, 757, 203, 10, 4, 447, 46, 85, 813, 3, 178, 11, 84, 75, 163, 1, 1484, 75, 2086, 120, 5, 557, 757, 142, 401, 141, 1357, 2730, 23, 212, 31, 8, 320, 1240, 892, 23, 666, 178, 11, 84, 75, 163, 1, 557, 757, 475, 104, 220, 4, 557, 757, 7, 25, 149, 8, 4, 475, 104, 109, 36, 259, 93, 1, 36, 69, 176, 14, 45, 212, 764, 1, 6, 78, 79, 3, 309, 351, 568, 29, 55, 51, 1688, 1120, 75, 56, 3, 64, 584, 64, 530, 1, 75, 79, 3, 38, 704, 1, 55, 51, 5, 142, 42, 1688, 468, 75, 157, 230, 8, 4, 875, 485, 20, 311, 371, 530, 286, 267, 45, 756, 468, 165, 176, 1754, 96, 23, 17, 116, 684, 150, 7, 58, 36, 495, 255, 919, 11, 51, 45, 963, 9, 493, 584, 1092, 311, 220, 4, 178, 11, 84, 557, 757, 8, 69, 1, 98, 595, 46, 638, 39, 557, 75, 163, 1, 703, 594, 8, 273, 494, 4, 447, 46, 1943, 1441, 279, 287, 1364, 9, 678, 625, 39, 714, 1, 1231, 1111, 579, 27, 249, 5, 714, 272, 230, 414, 33, 176, 17, 550, 767, 468, 949, 2, 17, 468, 75, 279, 3, 492, 814, 589, 475, 162, 162, 1, 1296, 274, 311, 884, 291, 32, 447, 46, 155, 8, 35, 53, 616, 281, 1277, 1823, 1, 789, 1135, 20, 689, 664, 2875, 2, 243, 172, 893, 80, 157, 1, 34, 240, 1390, 717, 82, 679, 579, 1, 132, 311, 809, 70, 39, 102, 349, 338, 107, 987, 3, 9, 860, 186, 88, 118, 23, 2, 107, 927, 785, 174, 7, 785, 1125, 66, 75, 55, 51, 5, 53, 1163, 72, 67, 2, 243, 1, 76, 12, 150, 405, 99, 446, 154, 14, 1, 679, 505, 155, 155, 178, 11, 84, 746, 153, 39, 557, 1, 14, 1393, 297, 33, 640, 1231, 1111, 579, 796, 1296, 274, 311, 884, 796, 132, 311, 809, 796, 122, 7, 1124, 3, 537, 2575, 563, 10, 1361, 2642, 2642, 1, 371, 171, 328, 278, 20, 493, 584, 1092, 311, 59, 121, 4, 796, 14, 671, 1, 828, 769, 220, 1325, 40, 9, 492, 182, 1447, 84, 20, 91, 1636, 84, 530, 176, 12, 38, 316, 220, 7, 796, 20, 796, 118, 492, 182, 1447, 84, 652, 3, 212, 764, 25, 295, 83, 142, 282, 145, 119, 8, 1670, 11, 94, 46, 683, 346, 460, 568, 105, 2, 29, 291, 32, 338, 104, 538, 214, 777, 11, 520, 225, 91, 1636, 84, 530, 4, 10, 51, 490, 862, 184, 39, 557, 1644, 56, 1, 54, 572, 53, 451, 1, 292, 751, 1472, 159, 279, 12, 320, 214, 88, 183, 884, 2, 243, 1, 202, 737, 2, 243, 1, 1193, 1472, 220, 4, 42, 694, 405, 879, 8, 342, 117, 69, 7, 734, 940, 694, 2200, 2279, 23, 243, 1, 39, 557, 178, 11, 84, 850, 36, 495, 255, 309, 351, 568, 1, 679, 579, 64, 584, 64, 530, 828, 769, 652, 7, 234, 344, 291, 29, 55, 51, 53, 160, 757, 50, 899, 27, 451, 485, 165, 39, 1469, 83, 4, 522, 392, 36, 889, 123, 97, 1037, 39, 557, 45, 1, 1734, 1067, 117, 56, 140, 222, 2, 468, 1, 160, 1104, 752, 561, 118, 875, 485, 679, 27, 2128, 174, 1, 292, 39, 75, 63, 178, 11, 84, 555, 164, 309, 351, 1, 1188, 1525, 638, 557, 256, 789, 81, 993, 946, 793, 562, 177, 4, 279, 451, 485, 165, 12, 254, 1, 1246, 1754, 294, 33, 9, 173, 207, 12, 281, 112, 1, 1715, 1220, 287, 311, 371, 530, 286, 64, 584, 64, 530, 282, 1380, 697, 800, 413, 412, 517, 38, 456, 239, 53, 63, 311, 371, 496, 572, 360, 141, 291, 32, 118, 23, 289, 2, 34, 311, 371, 530, 286, 3, 5, 343, 435, 81, 177, 64, 1, 81, 75, 372, 126, 176, 17, 625, 343, 135, 123, 81, 430, 53, 60, 64, 107, 84, 94, 5, 343, 226, 625, 343, 1, 298, 869, 456, 33, 176, 1279, 1, 610, 394, 506, 178, 11, 84, 39, 557, 2565, 1384, 76, 233, 1, 52, 415, 60, 828, 769, 59, 885, 9, 53, 60, 112, 56, 3, 1, 178, 11, 84, 39, 714, 8, 1752, 81, 2100, 12, 220, 16, 218, 557, 23, 17, 185, 85, 112, 81, 8, 756, 316, 155, 1102, 182, 1531, 451, 317, 328, 328, 53, 60, 220, 16, 731, 488, 12, 1, 358, 187, 717, 23, 95, 117, 59, 4, 2, 267, 49, 401, 1, 120, 266, 1688, 9, 654, 1292, 800, 178, 11, 84, 1, 71, 610, 59, 222, 166, 211, 77, 1660, 1025, 448, 7, 529, 635, 2473, 891, 1, 557, 367, 2974, 159, 8, 526, 367, 82, 3, 291, 29, 55, 51, 282, 875, 485, 446, 198, 80, 78, 1211, 3255, 174, 7, 529, 635, 264, 322, 287, 166, 64, 584, 64, 530, 220, 36, 1750, 392, 64, 530, 3, 45, 29, 1472, 1499, 1472, 287, 789, 77, 1048, 343, 131, 117, 10, 23, 17, 11, 17, 187, 59, 150, 1111, 69, 53, 1, 198, 49, 66, 444, 239, 882, 541, 275, 311, 215, 764, 1636, 311, 138, 12, 1008, 81, 779, 846, 834, 96, 58, 1463, 267, 45, 344, 567, 722, 347, 35, 78, 413, 413, 756, 468, 834, 96, 178, 11, 84, 1, 71, 610, 287, 81, 150, 738, 364, 220, 4, 2, 891, 151, 625, 343, 214, 75, 845, 662, 530, 23, 227, 364, 2, 288, 287, 166, 583, 172, 35, 8, 756, 432, 230, 220, 4, 847, 35, 202, 79, 25, 23, 2, 580, 1173, 1162, 405, 28, 198, 414, 287, 441, 67, 94, 118, 31, 150, 101, 178, 64, 584, 64, 530, 988, 94, 764, 1636, 311, 415, 79, 103, 3, 178, 1157, 985, 3183, 37, 2613, 779, 30, 8, 4, 1317, 159, 176, 29, 55, 51, 53, 142, 42, 7, 9, 471, 107, 819, 267, 1400, 392, 1, 146, 97, 7, 392, 412, 34, 1401, 35, 1, 71, 75, 613, 128, 3, 40, 2, 29, 61, 895, 209, 5, 202, 930, 175, 225, 64, 584, 64, 530, 324, 1041, 493, 1149, 328, 88, 215, 305, 493, 814, 797, 177, 64, 96, 16, 20, 764, 1636, 311, 218, 3202, 176, 625, 157, 168, 220, 7, 146, 97, 621, 503, 662, 530, 494, 7, 177, 64, 1372, 5, 1042, 66, 1, 96, 16, 82, 3, 621, 503, 504, 487, 493, 1149, 328, 88, 215, 1, 543, 69, 312, 159, 142, 42, 8, 157, 2, 14, 45, 1492, 327, 305, 9, 178, 11, 84, 61, 122, 1, 96, 16, 58, 4, 70, 39, 2, 209, 30, 166, 6, 2876, 4, 290, 289, 329, 169, 58, 28, 728, 40, 166, 6, 817, 287, 367, 394, 28, 178, 11, 84, 23, 576, 39, 557, 1, 36, 69, 418, 7, 793, 925, 832, 3, 1208, 2250, 585, 739]]\n"
     ]
    }
   ],
   "source": [
    "print(train_idlist_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练样本的文本矩阵转化为等长度的ndarray类型\n",
    "train_X = kr.preprocessing.sequence.pad_sequences(\n",
    "    train_idlist_list, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练样本的标签矩阵进行格式化\n",
    "labelEncoder = spl()\n",
    "train_y = labelEncoder.fit_transform(train_label_list)\n",
    "# 将训练集标签矩阵转化为独热编码格式\n",
    "train_Y = kr.utils.to_categorical(train_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除默认图形堆栈并重置全局默认图形\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope('input_layer'):\n",
    "    # 利用占位符创建索引概率分布矩阵\n",
    "    X_holder = tf.placeholder(tf.int32, [None, seq_length])\n",
    "    Y_holder = tf.placeholder(tf.float32, [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试样本映射的词向量矩阵\n",
    "test_idlist_list = list2vector(\n",
    "    word2id_dict,test_cont_list)\n",
    "# 将测试样本的文本矩阵转化为等长度的ndarray类型\n",
    "test_X = kr.preprocessing.sequence.pad_sequences(\n",
    "    test_idlist_list,seq_length)\n",
    "# 将测试样本的标签矩阵进行格式化\n",
    "test_y = labelEncoder.fit_transform(test_label_list)\n",
    "# 将测试集标签矩阵转化为独热编码格式\n",
    "test_Y = kr.utils.to_categorical(test_y, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、CNN模型训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN前向传播流程\n",
    "# 创建词向量的变量\n",
    "embedding = tf.get_variable('embedding',[vocab_size, embedding_dim])\n",
    "# 选取词向量索引矩阵对应的元素,作为输入层数据\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding,X_holder)\n",
    "with tf.name_scope('conv'):\n",
    "    # 创建一维卷积函数\n",
    "    conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size)\n",
    "    # 利用横向局部最大值进行池化\n",
    "    max_pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "with tf.name_scope('full_connect'):\n",
    "    # 创建全连接层\n",
    "    full_connect = tf.layers.dense(max_pooling, hidden_dim)\n",
    "    # 降采样，防止神经网络过拟合，保留75%神经元参与训练\n",
    "    full_connect_drop = tf.contrib.layers.dropout(full_connect,keep_prob=0.75)\n",
    "    # 利用relu函数进行全连接层激活处理\n",
    "    full_connect_activate = tf.nn.relu(full_connect_drop)\n",
    "with tf.name_scope('predict'):\n",
    "    # 将输出层前全连接层的输出进行训练，得出预测得分\n",
    "    softmax_pre = tf.layers.dense(full_connect_activate, num_classes)\n",
    "    # 将预测得分进行softmax激活处理，得到各种类别的预测概率\n",
    "    pred_Y = tf.nn.softmax(softmax_pre)\n",
    "\n",
    "# CNN后向传播流程\n",
    "with tf.name_scope('loss'):\n",
    "    # 计算输出标签与真实标签的交叉熵\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=Y_holder, logits=softmax_pre)\n",
    "    # 计算损失(交叉熵均值)误差均值\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar('loss',loss)\n",
    "with tf.name_scope('optimizer'):\n",
    "    # 利用Adam算法创建自动优化器,学习率为0.001\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # 对损失函数进行全局优化\n",
    "    train = optimizer.minimize(loss)\n",
    "merged = tf.summary.merge_all()\n",
    "# 获取真实标签与预测标签的元素布尔型矩阵\n",
    "isCorrect = tf.equal(tf.argmax(Y_holder, 1), tf.argmax(pred_Y, 1))\n",
    "# 将布尔型矩阵转化为0-1矩阵，并浮点化，并求预测准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(isCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-354071749faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# CNN前向传播流程\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 创建词向量的变量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embedding'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# 选取词向量索引矩阵对应的元素,作为输入层数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0membedding_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_holder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    859\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 861\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    862\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# CNN前向传播流程\n",
    "# 创建词向量的变量\n",
    "embedding = tf.get_variable('embedding',[vocab_size, embedding_dim])\n",
    "# 选取词向量索引矩阵对应的元素,作为输入层数据\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding,X_holder)\n",
    "# 创建一维卷积函数\n",
    "conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size)\n",
    "# 利用横向局部最大值进行池化\n",
    "max_pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "# 创建全连接层\n",
    "full_connect = tf.layers.dense(max_pooling, hidden_dim)\n",
    "# 降采样，防止神经网络过拟合，保留75%神经元参与训练\n",
    "full_connect_drop = tf.contrib.layers.dropout(full_connect,keep_prob=0.75)\n",
    "# 利用relu函数进行全连接层激活处理\n",
    "full_connect_activate = tf.nn.relu(full_connect_drop)\n",
    "# 将输出层前全连接层的输出进行训练，得出预测得分\n",
    "softmax_pre = tf.layers.dense(full_connect_activate, num_classes)\n",
    "# 将预测得分进行softmax激活处理，得到各种类别的预测概率\n",
    "pred_Y = tf.nn.softmax(softmax_pre)\n",
    "\n",
    "# CNN后向传播流程\n",
    "# 计算输出标签与真实标签的交叉熵\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    labels=Y_holder, logits=softmax_pre)\n",
    "# 计算损失(交叉熵均值)误差均值\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# 利用Adam算法创建自动优化器,学习率为0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# 对损失函数进行全局优化\n",
    "train = optimizer.minimize(loss)\n",
    "# 获取真实标签与预测标签的元素布尔型矩阵\n",
    "isCorrect = tf.equal(tf.argmax(Y_holder, 1), tf.argmax(pred_Y, 1))\n",
    "# 将布尔型矩阵转化为0-1矩阵，并浮点化，并求预测准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(isCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五、模型的训练与预测\n",
    "* 进行2000轮训练\n",
    "* 观察损失函数loss和预测准确率accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、创建模型保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存对象\n",
    "saver = tf.train.Saver()\n",
    "# 最优结果\n",
    "best_acc = 0\n",
    "# 创建保存训练模型的路径\n",
    "model_path = 'save_models/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "save_path = os.path.join(path,'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、会话初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加节点用于初始化全部变量\n",
    "init = tf.global_variables_initializer()\n",
    "# 创建会话\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、创建数据流图可视化实例\n",
    "* 利用tensorboard进行数据流图可视化\n",
    "* 查看命令：（进入主程序文件所在路径地址执行）tensorboard --logdir ./ --host localhost\n",
    "* 在浏览器中输入：localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建FileWriter实例，并传入当前会话加载的数据流图\n",
    "writer = tf.summary.FileWriter('./summary/linear-regression-1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、训练代码执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,11]\n\t [[node Placeholder_1 (defined at <ipython-input-119-8a7ab58dd663>:5)  = Placeholder[dtype=DT_FLOAT, shape=[?,11], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_1', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-119-8a7ab58dd663>\", line 5, in <module>\n    Y_holder = tf.placeholder(tf.float32, [None, num_classes])\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6251, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,11]\n\t [[node Placeholder_1 (defined at <ipython-input-119-8a7ab58dd663>:5)  = Placeholder[dtype=DT_FLOAT, shape=[?,11], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,11]\n\t [[{{node Placeholder_1}} = Placeholder[dtype=DT_FLOAT, shape=[?,11], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-c0e97b1ace1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     sess.run(train, feed_dict={\n\u001b[0;32m     10\u001b[0m         X_holder:batch_X, Y_holder:batch_Y})\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# 抽取样本观察训练效果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,11]\n\t [[node Placeholder_1 (defined at <ipython-input-119-8a7ab58dd663>:5)  = Placeholder[dtype=DT_FLOAT, shape=[?,11], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_1', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-119-8a7ab58dd663>\", line 5, in <module>\n    Y_holder = tf.placeholder(tf.float32, [None, num_classes])\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6251, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Python\\venvenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,11]\n\t [[node Placeholder_1 (defined at <ipython-input-119-8a7ab58dd663>:5)  = Placeholder[dtype=DT_FLOAT, shape=[?,11], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # 随机选取64个样本的索引\n",
    "    sel_index = random.sample(\n",
    "        list(range(len(train_y))), k=batch_size)\n",
    "    # 批量选择训练样本\n",
    "    batch_X = train_X[sel_index]\n",
    "    batch_Y = train_Y[sel_index]\n",
    "    # 运行训练模型\n",
    "    sess.run(train, feed_dict={\n",
    "        X_holder:batch_X, Y_holder:batch_Y})\n",
    "    rs=sess.run(merged)\n",
    "    writer.add_summary(rs, i)\n",
    "    # 抽取样本观察训练效果\n",
    "    i += 1\n",
    "    if i % 2 == 0:\n",
    "        # 随机选择200个样本进行训练\n",
    "        sel_index = random.sample(\n",
    "            list(range(len(train_y))), k=200)\n",
    "        batch_X = train_X[sel_index]\n",
    "        batch_Y = train_Y[sel_index]\n",
    "        # 获取损失值loss_val和准确度accuracy_val\n",
    "        loss_val,accuracy_val = sess.run(\n",
    "            [loss,accuracy], feed_dict={\n",
    "                X_holder:batch_X, Y_holder:batch_Y})\n",
    "        # 将最优的模型保存下来\n",
    "        if accuracy_val > best_acc:\n",
    "            best_acc = accuracy_val\n",
    "            saver.save(sess=sess, save_path=save_path)\n",
    "        # 打印损失值和准确度\n",
    "        print('第{}轮训练，loss值:{:.4f}，accuracy值：{:.4f}'.format(\n",
    "            i,loss_val,accuracy_val))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六、混淆矩阵\n",
    "* 通过混淆矩阵评估模型的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关工具包\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3572\n"
     ]
    }
   ],
   "source": [
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、数据格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义批量预测函数，将结果转化为ndarray类型\n",
    "def predictAll(test_X, batch_size=100):\n",
    "    pred_val_list = []\n",
    "    for i in range(0,len(test_X),batch_size):\n",
    "        select_X = test_X[i:i+batch_size]\n",
    "        pred_val = sess.run(pred_Y, {X_holder:select_X})\n",
    "        pred_val_list.extend(pred_val)\n",
    "    return np.array(pred_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、获取混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>体育</th>\n",
       "      <th>健康</th>\n",
       "      <th>军事</th>\n",
       "      <th>奥运</th>\n",
       "      <th>女人</th>\n",
       "      <th>房产</th>\n",
       "      <th>教育</th>\n",
       "      <th>文化</th>\n",
       "      <th>汽车</th>\n",
       "      <th>科技</th>\n",
       "      <th>财经</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>体育</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>健康</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>军事</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>奥运</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>女人</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>房产</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>教育</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文化</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>汽车</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>科技</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>财经</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     体育  健康  军事   奥运   女人  房产   教育  文化   汽车  科技    财经\n",
       "体育  921   0   0   37    3   0    0   0    2   0     2\n",
       "健康    0  27   0    0   13   0    1   1    0   0     0\n",
       "军事    1   0  56    2    1   0    0   1    0   0     4\n",
       "奥运   41   0   0  214    3   0    4   0    1   0    11\n",
       "女人    2   0   1    0  315   0    1   1    0   0     2\n",
       "房产    1   0   0    1    0  51    1   1    0   1    17\n",
       "教育    1   0   0    1    3   0  229   0    0   0     1\n",
       "文化    2   0   5    3    8   0    0  18    1   0     1\n",
       "汽车    2   0   1    2    1   0    1   0  266   0     5\n",
       "科技    3   0   0    0    2   1    3   0    1  58    19\n",
       "财经    2   0   3    9    8   4    3   0    2   6  1156"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运行模型进行预测\n",
    "Y = predictAll(test_X)\n",
    "# 获取局部最大可能预测值的位置矩阵\n",
    "y = np.argmax(Y, axis=1)\n",
    "# 格式化预测标签列表\n",
    "pred_label_list = labelEncoder.inverse_transform(y)\n",
    "# 获取混淆矩阵\n",
    "content_matrix = confusion_matrix(test_label_list,pred_label_list)\n",
    "pd.DataFrame(content_matrix,columns=labelEncoder.classes_,\n",
    "            index=labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、预测分析小结\n",
    "* 绝大多数样本预测都比较准确。\n",
    "* 奥运和体育出现交叉，说明二者有较大的关联性，这与事实相符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 七、报告分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、定义并执行评估数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "\n",
    "# 定义模型评估函数\n",
    "def eval_model(true_y,pred_y,labels):\n",
    "    # 计算每个类别的测准率，召回率，f1得分\n",
    "    pred,recall,f1,s = prf(true_y, pred_y)\n",
    "    # 计算上面各参数的总体值\n",
    "    tt_pred = np.average(pred, weights=s)\n",
    "    tt_recall = np.average(recall, weights=s)\n",
    "    tt_f1 = np.average(f1, weights=s)\n",
    "    tt_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        'Label':labels,\n",
    "        'Prediction':pred,\n",
    "        'Recall':recall,\n",
    "        'F1':f1,\n",
    "        'Support':s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        'Label':['总体'],\n",
    "        'Prediction':tt_pred,\n",
    "        'Recall':tt_recall,\n",
    "        'F1':tt_f1,\n",
    "        'Support':tt_s        \n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])[['Label','Prediction','Recall','F1','Support']]\n",
    "    return res,res1,res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "res,res1,res2 = eval_model(test_label_list, \n",
    "           pred_label_list, labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、可视化评估数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label  Prediction    Recall        F1  Support\n",
      "0      体育    0.943648  0.954404  0.948995      965\n",
      "1      健康    1.000000  0.642857  0.782609       42\n",
      "2      军事    0.848485  0.861538  0.854962       65\n",
      "3      奥运    0.795539  0.781022  0.788214      274\n",
      "4      女人    0.882353  0.978261  0.927835      322\n",
      "5      房产    0.910714  0.698630  0.790698       73\n",
      "6      教育    0.942387  0.974468  0.958159      235\n",
      "7      文化    0.818182  0.473684  0.600000       38\n",
      "8      汽车    0.974359  0.956835  0.965517      278\n",
      "9      科技    0.892308  0.666667  0.763158       87\n",
      "10     财经    0.949097  0.968986  0.958938     1193\n",
      "999    总体    0.926561  0.926932  0.924823     3572\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 八、项目总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 本项目以“搜狗实验室数据”的真实新闻数据为数据来源，预测结果具有客观性。\n",
    "* 本项目全流程展示了数据预处理、数据清洗、模型的建立及评估的基本思路和实现代码。\n",
    "* 本项目利用CNN算法建立模型，经过2000轮训练获得的准确率高达99%，可以投入实际应用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvenv",
   "language": "python",
   "name": "venvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
